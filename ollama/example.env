# Enable CUDA support
OLLAMA_CUDA=1
# Enable optimized attention mechanism for faster processing on GPU
# Recommended for Nvidia GPUs to improve token generation speed
OLLAMA_FLASH_ATTENTION=1
# Keep models loaded in memory (default is 5m)
# Increase this if you're using the model frequently
OLLAMA_KEEP_ALIVE=30m